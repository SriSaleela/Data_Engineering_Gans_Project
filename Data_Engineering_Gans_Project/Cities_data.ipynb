{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7XGRW58Onq4",
    "outputId": "06035ada-1ef2-4c65-a285-61994611f6aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/Priyatam/opt/anaconda3/lib/python3.9/site-packages (4.12.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/Priyatam/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "# colab has an older version of beautifulsoup by default\n",
    "# here we upgrade it\n",
    "# if you are working on your own computer, you can probably comment this step out and skip it\n",
    "!pip install --upgrade beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "09E5mZ6mjM-J"
   },
   "outputs": [],
   "source": [
    "# 1. import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EfulSS0rj9Gd"
   },
   "outputs": [],
   "source": [
    "# 2. find url and store it in a variable\n",
    "url = \"https://en.wikipedia.org/wiki/Berlin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ToaJ81GkC0r",
    "outputId": "4973d63e-4efe-4105-f8da-be2f6be04be5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. download html with a get request\n",
    "response = requests.get(url)\n",
    "response.status_code # 200 status code means OK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bBB9cK_1kHeK"
   },
   "outputs": [],
   "source": [
    "# 4.1. parse html (create the 'soup')\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "# 4.2. check that the html code looks like it should\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEPylwtLkRz6",
    "outputId": "d14a65cd-517c-454a-8608-1c7f9a53c8a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\"><span class=\"mw-page-title-main\">Berlin</span></h1>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. retrieve/extract the desired info (here, you'll paste the \"Selector\" you copied before to get the element that belongs to the top movie)\n",
    "\n",
    "# let's first try to get the name of the city\n",
    "# by copying the selector we can see that it has the id firstHeading (it also has a class by the same name!)\n",
    "soup.select(\"#firstHeading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "xC2gDSscWtyS",
    "outputId": "010a8325-7cc1-4b6f-f25f-64b7075c857c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Berlin'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select(\"#firstHeading\")[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "zqaG8u2YX96Z",
    "outputId": "a4d6af80-fcf8-4ce1-bb67-dc85dd5b7ce9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Germany'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use this class, infobox-data, to target the information country\n",
    "soup.select(\".infobox-data\")[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hdVBTcmTIwh8"
   },
   "outputs": [],
   "source": [
    "#soup.select(\".infobox-data\")[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5NbgdoeYKIo"
   },
   "source": [
    "Now we just carry on exploring the html, finding classes, ids, and selectors to target the information we need. Hopefully these classes and selectors will be universal across all cities on wikipedia, but it is likely that they will change in a few places, and we will have to try to make our code robust to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6ZaT16qaP2yJ"
   },
   "outputs": [],
   "source": [
    "def recreate_wiki(cities):\n",
    "  # empty list that will be filled with one dictionary of information per city\n",
    "  list_for_df = []\n",
    "  \n",
    "  # begin a for loop to create a dictionary of information for each city\n",
    "  for city in cities:\n",
    "    # we can use the universal nature of wikipedias urls to our advantage here\n",
    "    # all of the urls are the same besides the city name\n",
    "    url = f'https://en.wikipedia.org/wiki/{city}'\n",
    "\n",
    "    # here we make our soup for the city\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "    # here we initialise our empty dictionary for the city\n",
    "    response_dict = {}\n",
    "\n",
    "    # here we fill the dictionary with information using the ids, classes, and selectors that we found in the html\n",
    "    response_dict['city'] = soup.select(\".firstHeading\")[0].get_text()\n",
    "    response_dict['country'] = soup.select(\".infobox-data\")[0].get_text()\n",
    "    response_dict['latitude'] = soup.select(\".latitude\")[0].get_text()\n",
    "    response_dict['longitude'] = soup.select(\".longitude\")[0].get_text()\n",
    "    # not all of the wikipedia pages contain elevation, look at Hamburg\n",
    "    # the if clause means that our code can continue and won't stop at this hurdle\n",
    "    if soup.select_one('.infobox-label:-soup-contains(\"Elevation\")'):\n",
    "      response_dict['elevation'] = soup.select_one('.infobox-label:-soup-contains(\"Elevation\")').find_next(class_='infobox-data').get_text()\n",
    "    response_dict['website'] = soup.select_one('.infobox-label:-soup-contains(\"Website\")').find_next(class_='infobox-data').get_text()\n",
    "    if soup.select_one('th.infobox-header:-soup-contains(\"Population\")'):\n",
    "        response_dict['population'] = soup.select_one('th.infobox-header:-soup-contains(\"Population\")').parent.find_next_sibling().find(text=re.compile(r'\\d+'))\n",
    "    \n",
    "    # add our dictionary for the city to list_for_df\n",
    "    list_for_df.append(response_dict)\n",
    "  \n",
    "  # make the DataFrame\n",
    "  cities_df = pd.DataFrame(list_for_df)\n",
    "\n",
    "  # fixing latitude\n",
    "  cities_df['latitude'] = cities_df['latitude'].str.split('″').str[0].str.replace('°', '.', regex=False).str.replace('′', '', regex=False)\n",
    "  # fixing longitude\n",
    "  cities_df['longitude'] = cities_df['longitude'].str.split('″').str[0].str.replace('°', '.', regex=False).str.replace('′', '', regex=False)\n",
    "  # fixing elevation\n",
    "  cities_df.insert(4, 'elevation_in_meters', cities_df['elevation'].str.split('m').str[0].str.strip())\n",
    "\n",
    "  # return the DataFrame\n",
    "  return cities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "vPj5D3KfTZk2",
    "outputId": "41f95d79-a85f-47cb-d03d-5b4ee98a0814",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h0/80_f4fw53yx0ftl50j43_nb80000gn/T/ipykernel_6005/3613996428.py:29: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  response_dict['population'] = soup.select_one('th.infobox-header:-soup-contains(\"Population\")').parent.find_next_sibling().find(text=re.compile(r'\\d+'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation_in_meters</th>\n",
       "      <th>elevation</th>\n",
       "      <th>website</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Berlin</td>\n",
       "      <td>Germany</td>\n",
       "      <td>52.3112</td>\n",
       "      <td>13.2418</td>\n",
       "      <td>34</td>\n",
       "      <td>34 m (112 ft)</td>\n",
       "      <td>berlin.de</td>\n",
       "      <td>3,677,472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Germany</td>\n",
       "      <td>53.33N</td>\n",
       "      <td>10.00E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hamburg.com</td>\n",
       "      <td>1,906,411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>London</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>51.3026</td>\n",
       "      <td>0.739</td>\n",
       "      <td>36 ft (11</td>\n",
       "      <td>36 ft (11 m)</td>\n",
       "      <td>www.london.gov.uk</td>\n",
       "      <td>8,799,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manchester</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>53.2846</td>\n",
       "      <td>2.1443</td>\n",
       "      <td>38</td>\n",
       "      <td>38 m (125 ft)</td>\n",
       "      <td>manchester.gov.uk</td>\n",
       "      <td>551,938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Spain</td>\n",
       "      <td>41.23N</td>\n",
       "      <td>2.11E</td>\n",
       "      <td>12</td>\n",
       "      <td>12 m (39 ft)</td>\n",
       "      <td>www.barcelona.cat</td>\n",
       "      <td>1,620,343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         city          country latitude longitude elevation_in_meters  \\\n",
       "0      Berlin          Germany  52.3112   13.2418                  34   \n",
       "1     Hamburg          Germany   53.33N    10.00E                 NaN   \n",
       "2      London   United Kingdom  51.3026     0.739           36 ft (11   \n",
       "3  Manchester   United Kingdom  53.2846    2.1443                  38   \n",
       "4   Barcelona            Spain   41.23N     2.11E                  12   \n",
       "\n",
       "       elevation             website population  \n",
       "0  34 m (112 ft)           berlin.de  3,677,472  \n",
       "1            NaN         hamburg.com  1,906,411  \n",
       "2   36 ft (11 m)  www.london.gov.uk   8,799,800  \n",
       "3  38 m (125 ft)   manchester.gov.uk    551,938  \n",
       "4   12 m (39 ft)  www.barcelona.cat   1,620,343  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_cities = ['Berlin', 'Hamburg', 'London', 'Manchester', 'Barcelona']\n",
    "recreate_wiki(list_of_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
